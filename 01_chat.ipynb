{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61133469",
   "metadata": {},
   "source": [
    "## LangChain-å°è©±(Chat)ç¯„ä¾‹\n",
    "2025/06/29 è˜‡å½¥åº­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042172b",
   "metadata": {},
   "source": [
    "1. å–®è¼ªå°è©±(Single-turn Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7677ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€å€‹å¤§å‹èªè¨€æ¨¡å‹ï¼Œç”± Google è¨“ç·´ã€‚æˆ‘å¯ä»¥æ ¹æ“šä½ çš„è¦æ±‚æä¾›å„ç¨®å…§å®¹ï¼Œä¾‹å¦‚ï¼š\n",
      "\n",
      "*   **å›ç­”å•é¡Œï¼š** è©¦è‘—å›ç­”ä½ æå‡ºçš„å•é¡Œï¼Œç„¡è«–æ˜¯é—œæ–¼çŸ¥è­˜ã€å‰µæ„ã€æˆ–æƒ³åƒåŠ›æ–¹é¢ã€‚\n",
      "*   **ç”Ÿæˆæ–‡æœ¬ï¼š** å¯«ä¸åŒé¡å‹çš„æ–‡æœ¬ï¼Œæ¯”å¦‚è©©æ­Œã€ç¨‹å¼ç¢¼ã€ à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿã€éŸ³æ¨‚ä½œå“ã€éƒµä»¶ã€ä¿¡ä»¶ç­‰ã€‚\n",
      "*   **ç¿»è­¯èªè¨€ï¼š** å°‡æ–‡æœ¬å¾ä¸€ç¨®èªè¨€ç¿»è­¯æˆå¦ä¸€ç¨®èªè¨€ã€‚\n",
      "*   **ç¸½çµæ–‡æœ¬ï¼š** æå–æ–‡æœ¬ä¸­çš„é—œéµä¿¡æ¯ã€‚\n",
      "*   **é‡è¤‡æ–‡æœ¬ï¼š** è®“æˆ‘çŸ¥é“è¦é‡è¤‡ä¸€æ¬¡æ–‡æœ¬ã€‚\n",
      "\n",
      "ç°¡å–®ä¾†èªªï¼Œæˆ‘æ˜¯ä¸€å€‹ç”¨èªè¨€ä¾†å¹«åŠ©ä½ çš„äººå·¥æ™ºèƒ½ã€‚\n",
      "\n",
      "ä½ æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘èŠçš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"ä½ æ˜¯èª°?\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca32b89",
   "metadata": {},
   "source": [
    "2. ç³»çµ±è¨Šæ¯æ§åˆ¶(System Message Control)\n",
    "* SystemMessage\n",
    "    * ç”¨é€”ï¼šæä¾›ç³»çµ±è§’è‰²è¨­å®šæˆ–å°æ¨¡å‹è¡Œç‚ºçš„æŒ‡ç¤ºã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†å‘Šè¨´æ¨¡å‹æ‡‰è©²æ‰®æ¼”ä»€éº¼æ¨£çš„è§’è‰²æˆ–éµå¾ªå“ªäº›è¦å‰‡ã€‚\n",
    "* HumanMessage\n",
    "    * ç”¨é€”ï¼šæ¨¡æ“¬ä½¿ç”¨è€…çš„è¼¸å…¥ã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†å‚³éä½¿ç”¨è€…çš„å•é¡Œæˆ–è«‹æ±‚ã€‚\n",
    "* AIMessage\n",
    "    * ç”¨é€”ï¼šæ¨¡æ“¬ AI çš„å›æ‡‰ã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†è¨˜éŒ„å°è©±ä¸­ AI æ‰€å›è¦†çš„å…§å®¹ï¼Œè®“å¾ŒçºŒå°è©±æœ‰ä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bd17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are asking â€œWho are you?â€ in Chinese. \n",
      "\n",
      "The correct translation is: **Who are you?** or **You are who?**\n",
      "\n",
      "Itâ€™s a straightforward question. ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from Chinese into English\"),\n",
    "    HumanMessage(\"ä½ æ˜¯èª°?\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc2188",
   "metadata": {},
   "source": [
    "3 å…·è¨˜æ†¶çš„å¤šè¼ªå°è©±(Multi-turn Chat with Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacd69f",
   "metadata": {},
   "source": [
    "3.1 å¤šè¼ªå°è©±ç„¡è¨˜æ†¶æ€§ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f33505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ç¬¬1æ¬¡å›ç­” ----------\n",
      "\n",
      "å¥½çš„ï¼Œæˆ‘è¨˜ä½äº†ï¼š\n",
      "\n",
      "A=ç‹—\n",
      "B=è²“\n",
      "C=é³¥\n",
      "\n",
      "ç¾åœ¨æˆ‘æº–å‚™å¥½æ¥æ”¶ä½ çš„æŒ‡ä»¤äº†ã€‚ ğŸ˜Š\n",
      "\n",
      "\n",
      "---------- ç¬¬2æ¬¡å›ç­” ----------\n",
      "\n",
      "åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ŒA, B, C é€šå¸¸ä»£è¡¨ **è˜‹æœã€ç•ªèŒ„å’Œè»Šå­**ã€‚ \n",
      "\n",
      "é€™æ˜¯ä¸€å€‹å¸¸è¦‹çš„åœ–åƒè¬èªï¼Œéœ€è¦æ ¹æ“šä¸Šä¸‹æ–‡ä¾†æ¨æ–·ç­”æ¡ˆã€‚\n",
      "\n",
      "ä½ æä¾›çš„æ–‡å­—ä¸­æ²’æœ‰æ˜ç¢ºæŒ‡å‡ºæ˜¯å“ªå€‹åœ–ç‰‡ï¼Œæ‰€ä»¥éœ€è¦æ›´å¤šè³‡è¨Šæ‰èƒ½åˆ¤æ–·ç­”æ¡ˆã€‚\n",
      "\n",
      "å¦‚æœä½ èƒ½æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼Œæˆ‘å¯ä»¥æ›´æº–ç¢ºåœ°å›ç­”ä½ çš„å•é¡Œã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"è«‹è¨˜ä½ä»£è™Ÿ: A=ç‹— B=è²“ C=é³¥\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(\"\\n---------- ç¬¬1æ¬¡å›ç­” ----------\\n\")\n",
    "print(res.content)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"è«‹å•A, B, Cåˆ†åˆ¥ä»£è¡¨ä»€éº¼?\")\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(\"\\n---------- ç¬¬2æ¬¡å›ç­” ----------\\n\")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab98d2",
   "metadata": {},
   "source": [
    "3.2 å°è©±å…·è¨˜æ†¶æ€§ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99689e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œæ ¹æ“šä½ æä¾›çš„ä»£è™Ÿï¼ŒAä»£è¡¨**ç‹—**ï¼ŒBä»£è¡¨**è²“**ï¼ŒCä»£è¡¨**é³¥**ã€‚\n",
      "\n",
      "å¾ˆé«˜èˆˆèƒ½å¹«åˆ°ä½ ï¼ ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"è«‹è¨˜ä½ä»£è™Ÿ: A=ç‹— B=è²“ C=é³¥\"),\n",
    "    AIMessage(content=\"å¥½ï¼Œæˆ‘è¨˜ä½äº† A=ç‹— B=è²“ C=é³¥\"),  # æ¨¡æ“¬ AI å›æ‡‰ å¹«åŠ©æ¨¡å‹è¨˜ä½å‰ä¸€è¼ªå…§å®¹\n",
    "    HumanMessage(content=\"è«‹å•A, B, Cåˆ†åˆ¥ä»£è¡¨ä»€éº¼?\"),  # æ–°ä¸€è¼ªè¼¸å…¥\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176c600",
   "metadata": {},
   "source": [
    "3.3 ç”¨ ConversationBufferMemory å»ºç«‹å¸¶è¨˜æ†¶çš„ Chat æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75de1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1184\\1558259831.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ç¬¬1æ¬¡ ----------\n",
      "\n",
      "å¥½çš„ï¼Œæˆ‘è¨˜ä½äº†ã€‚\n",
      "\n",
      "A=ç‹—\n",
      "B=è²“\n",
      "C=é³¥\n",
      "\n",
      "æˆ‘ç¾åœ¨èƒ½å¤ ç†è§£ä¸¦ä½¿ç”¨ä»£è™Ÿ Aã€Bã€C çš„æ„æ€ã€‚\n",
      "\n",
      "\n",
      "---------- ç¬¬2æ¬¡ ----------\n",
      "\n",
      "A ä»£è¡¨ **ç‹—**ï¼Œ B ä»£è¡¨ **è²“**ï¼Œ C ä»£è¡¨ **é³¥**ã€‚\n",
      "\n",
      "é‡è¤‡ç¢ºèªï¼Œæˆ‘ç†è§£äº†ï¼ ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# å»ºç«‹ Ollama æ¨¡å‹\n",
    "llm = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "# å»ºç«‹è¨˜æ†¶é«”\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "# æ‰‹å‹•æ¨¡æ“¬å°è©±è¼ªæ¬¡\n",
    "def ask(input_text):\n",
    "    # è¼‰å…¥è¨˜æ†¶è¨Šæ¯\n",
    "    history = memory.chat_memory.messages\n",
    "    # åŠ ä¸Šç•¶å‰äººé¡è¼¸å…¥\n",
    "    messages = history + [HumanMessage(content=input_text)]\n",
    "    # å‘¼å«æ¨¡å‹\n",
    "    response = llm.invoke(messages)\n",
    "    # å°‡æ–°å°è©±åŠ å…¥è¨˜æ†¶\n",
    "    memory.chat_memory.add_user_message(input_text)\n",
    "    memory.chat_memory.add_ai_message(response.content)\n",
    "    return response.content\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"\\n---------- ç¬¬1æ¬¡ ----------\\n\")\n",
    "print(ask(\"è«‹è¨˜ä½ä»£è™Ÿ: A=ç‹— B=è²“ C=é³¥\"))\n",
    "\n",
    "print(\"\\n---------- ç¬¬2æ¬¡ ----------\\n\")\n",
    "print(ask(\"è«‹å•A, B, Cåˆ†åˆ¥ä»£è¡¨ä»€éº¼?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813750",
   "metadata": {},
   "source": [
    "3.4 åˆ©ç”¨ Streamlit å¿«é€Ÿæ­ä¸€å€‹å°è©±æœå‹™\n",
    "\n",
    "ç¨‹å¼å·²å¯«å¥½åœ¨`chat_app.py`ï¼Œè«‹åŸ·è¡Œ`streamlit run chat_app.py`æŒ‡ä»¤ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
