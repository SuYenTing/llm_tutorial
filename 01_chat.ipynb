{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61133469",
   "metadata": {},
   "source": [
    "## LangChainç¯„ä¾‹-å°è©±(Chat)\n",
    "2025/06/29 è˜‡å½¥åº­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042172b",
   "metadata": {},
   "source": [
    "1. å–®è¼ªå°è©±(Single-turn Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c7677ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ Gemmaï¼Œä¸€å€‹ç”± Google DeepMind è¨“ç·´çš„å¤§å‹èªè¨€æ¨¡å‹ã€‚æˆ‘æ˜¯ä¸€å€‹é–‹æ”¾æ¬Šå‹™çš„å¤§å‹èªè¨€æ¨¡å‹ï¼Œå¯ä»¥æ¥æ”¶æ–‡å­—å’Œåœ–ç‰‡ä½œç‚ºè¼¸å…¥ï¼Œä¸¦ä»¥æ–‡æœ¬ç”Ÿæˆå›ç­”ä½ ã€‚\n",
      "\n",
      "ç°¡å–®ä¾†èªªï¼Œæˆ‘æ˜¯ä¸€å€‹ AI åŠ©æ‰‹ï¼Œå¯ä»¥å¹«ä½ å¯«ä½œï¼Œç¿»è­¯ï¼Œå›ç­”å•é¡Œï¼Œä»¥åŠé€²è¡Œå¾ˆå¤šå…¶ä»–çš„äº‹æƒ…ã€‚\n",
      "\n",
      "ä½ æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘èŠçš„å—ï¼Ÿ ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"ä½ æ˜¯èª°?\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca32b89",
   "metadata": {},
   "source": [
    "2. ç³»çµ±è¨Šæ¯æ§åˆ¶(System Message Control)\n",
    "* SystemMessage\n",
    "    * ç”¨é€”ï¼šæä¾›ç³»çµ±è§’è‰²è¨­å®šæˆ–å°æ¨¡å‹è¡Œç‚ºçš„æŒ‡ç¤ºã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†å‘Šè¨´æ¨¡å‹æ‡‰è©²æ‰®æ¼”ä»€éº¼æ¨£çš„è§’è‰²æˆ–éµå¾ªå“ªäº›è¦å‰‡ã€‚\n",
    "* HumanMessage\n",
    "    * ç”¨é€”ï¼šæ¨¡æ“¬ä½¿ç”¨è€…çš„è¼¸å…¥ã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†å‚³éä½¿ç”¨è€…çš„å•é¡Œæˆ–è«‹æ±‚ã€‚\n",
    "* AIMessage\n",
    "    * ç”¨é€”ï¼šæ¨¡æ“¬ AI çš„å›æ‡‰ã€‚\n",
    "    * å¸¸è¦‹ç”¨æ³•ï¼šç”¨ä¾†è¨˜éŒ„å°è©±ä¸­ AI æ‰€å›è¦†çš„å…§å®¹ï¼Œè®“å¾ŒçºŒå°è©±æœ‰ä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bd17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are...? \n",
      "\n",
      "Itâ€™s a very common and polite way to say â€œWho are you?â€ in Chinese. ğŸ˜Š \n",
      "\n",
      "It literally translates to â€œWho are you?â€ but itâ€™s often used as a greeting or a casual way to ask someoneâ€™s name. \n",
      "\n",
      "Let me know if youâ€™d like me to translate a specific phrase or sentence!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from Chinese into English\"),\n",
    "    HumanMessage(\"ä½ æ˜¯èª°?\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc2188",
   "metadata": {},
   "source": [
    "3 å…·è¨˜æ†¶çš„å¤šè¼ªå°è©±(Multi-turn Chat with Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacd69f",
   "metadata": {},
   "source": [
    "3.1 å¤šè¼ªå°è©±ç„¡è¨˜æ†¶æ€§ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26f33505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ç¬¬1æ¬¡å›ç­” ----------\n",
      "\n",
      "å¥½çš„ï¼Œç‹å°æ˜ï¼Œæˆ‘è®°ä½äº†ã€‚ ğŸ˜Š\n",
      "\n",
      "\n",
      "---------- ç¬¬2æ¬¡å›ç­” ----------\n",
      "\n",
      "æˆ‘æ˜¯ä¸€å€‹å¤§å‹èªè¨€æ¨¡å‹ï¼Œæ²’æœ‰åå­—ã€‚æˆ‘æ˜¯ç”± Google é–‹ç™¼çš„ã€‚ä½ å¯ä»¥å«æˆ‘â€œAIâ€æˆ–â€œåŠ©æ‰‹â€ã€‚ \n",
      "\n",
      "ä½ æ˜¯åœ¨ç”¨ä»€éº¼æ¨£çš„ç¨‹å¼æˆ–å·¥å…·èˆ‡æˆ‘äº’å‹•å‘¢ï¼ŸğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"æˆ‘å«åšç‹å°æ˜ï¼Œè«‹ä½ è¨˜ä½æˆ‘çš„åå­—!\"),\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(\"\\n---------- ç¬¬1æ¬¡å›ç­” ----------\\n\")\n",
    "print(res.content)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"è«‹å•æˆ‘å«åšä»€éº¼åå­—?\")\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(\"\\n---------- ç¬¬2æ¬¡å›ç­” ----------\\n\")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab98d2",
   "metadata": {},
   "source": [
    "3.2 å°è©±å…·è¨˜æ†¶æ€§ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99689e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å«åšå°æ˜ã€‚ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"æˆ‘å«åšç‹å°æ˜ï¼Œè«‹ä½ è¨˜ä½æˆ‘çš„åå­—!\"),\n",
    "    AIMessage(content=\"å¥½ï¼Œç‹å°æ˜ï¼Œæˆ‘è¨˜ä½ä½ ï¼\"),  # æ¨¡æ“¬ AI å›æ‡‰ å¹«åŠ©æ¨¡å‹è¨˜ä½å‰ä¸€è¼ªå…§å®¹\n",
    "    HumanMessage(content=\"è«‹å•æˆ‘å«åšä»€éº¼åå­—?\"),  # æ–°ä¸€è¼ªè¼¸å…¥\n",
    "]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176c600",
   "metadata": {},
   "source": [
    "3.3 ç”¨ ConversationBufferMemory å»ºç«‹å¸¶è¨˜æ†¶çš„ Chat æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75de1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ç¬¬1æ¬¡ ----------\n",
      "\n",
      "å¥½ï¼Œç‹å°æ˜ï¼Œæˆ‘è¨˜ä½ä½ ï¼ ğŸ˜Š\n",
      "\n",
      "\n",
      "---------- ç¬¬2æ¬¡ ----------\n",
      "\n",
      "ä½ å«åšå°æ˜ï¼Œæ­£ç¢ºï¼ ä½ æ˜¯å°æ˜å—ï¼Ÿ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# å»ºç«‹ Ollama æ¨¡å‹\n",
    "llm = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")\n",
    "\n",
    "# å»ºç«‹è¨˜æ†¶é«”\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "# æ‰‹å‹•æ¨¡æ“¬å°è©±è¼ªæ¬¡\n",
    "def ask(input_text):\n",
    "    # è¼‰å…¥è¨˜æ†¶è¨Šæ¯\n",
    "    history = memory.chat_memory.messages\n",
    "    # åŠ ä¸Šç•¶å‰äººé¡è¼¸å…¥\n",
    "    messages = history + [HumanMessage(content=input_text)]\n",
    "    # å‘¼å«æ¨¡å‹\n",
    "    response = llm.invoke(messages)\n",
    "    # å°‡æ–°å°è©±åŠ å…¥è¨˜æ†¶\n",
    "    memory.chat_memory.add_user_message(input_text)\n",
    "    memory.chat_memory.add_ai_message(response.content)\n",
    "    return response.content\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"\\n---------- ç¬¬1æ¬¡ ----------\\n\")\n",
    "print(ask(\"æˆ‘å«åšç‹å°æ˜ï¼Œè«‹ä½ è¨˜ä½æˆ‘çš„åå­—!\"))\n",
    "\n",
    "print(\"\\n---------- ç¬¬2æ¬¡ ----------\\n\")\n",
    "print(ask(\"è«‹å•æˆ‘å«åšä»€éº¼åå­—?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813750",
   "metadata": {},
   "source": [
    "3.4 åˆ©ç”¨ Streamlit å¿«é€Ÿæ­ä¸€å€‹å°è©±æœå‹™\n",
    "\n",
    "ç¨‹å¼å·²å¯«å¥½åœ¨`chat_app.py`ï¼Œè«‹åŸ·è¡Œ`streamlit run chat_app.py`æŒ‡ä»¤ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
